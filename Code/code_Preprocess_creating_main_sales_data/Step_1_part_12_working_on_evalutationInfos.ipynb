{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import ast\n",
    "import json\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "## ==================================================\n",
    "# Boliga_propertySales_evaluationInfo.csv\n",
    "## ==================================================\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----- read in the file : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'D:\\Thesis\\Properties\\Denmark\\RE_due_scraping_properties\\Boliga_dk\\Creating_main_dataset_for_sales_data\\Data_split\\12_evalutationInfos\\Boliga_propertySales_evalutationInfos_.csv'\n",
    "# data_evaluationInfos = pd.read_csv(path, encoding='utf-8',low_memory=False)\n",
    "# data_evaluationInfos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_evaluationInfos['RAW_dictionary_evaluationInfos'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------- Convert \"RAW_dictionary_salesInfos\" column to dictionary object                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_json(x):\n",
    "#     try:\n",
    "#         if isinstance(x, str) and x.strip() != \"\" and not pd.isna(x):\n",
    "#             return ast.literal_eval(x)\n",
    "#             # Alternatively, you can use json.loads as follows:\n",
    "#             # return json.loads(x.replace(\"'\", '\"'))\n",
    "#         else:\n",
    "#             return x\n",
    "#     except (ValueError, SyntaxError) as e:\n",
    "#         print(f\"Error decoding JSON: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Apply the decoding function to the specified column\n",
    "# data_evaluationInfos['RAW_dictionary_evaluationInfos'] = data_evaluationInfos['RAW_dictionary_evaluationInfos'].apply(decode_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_evaluationInfos['RAW_dictionary_evaluationInfos'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!! Data to big for RAM memory !!!!!!!\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "# PLAN: \n",
    "\n",
    "* Read in the file in chunks of 1000 rows per chunk (so 1038000/10000 = 104 chunks)\n",
    "* ONLY READ the main ID columns and \"RAW_dictionary_evaluationInfos\" column\n",
    "* for each chunk as a dataframe convert column \"RAW_dictionary_evaluationInfos\" into a dictionary object \n",
    "* then expand the column into rows i.e. 1 row = 1 dictionary\n",
    "* then noramlize all dictionaries into columns\n",
    "* Finally write result to csv file and move on to next chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your file path\n",
    "file_path = r'D:\\Thesis\\Properties\\Denmark\\RE_due_scraping_properties\\Boliga_dk\\Creating_main_dataset_for_sales_data\\Data_split\\12_evalutationInfos\\Boliga_propertySales_evalutationInfos_.csv'\n",
    "\n",
    "result_path = r'D:\\Thesis\\Properties\\Denmark\\RE_due_scraping_properties\\Boliga_dk\\Creating_main_dataset_for_sales_data\\Data_split\\12_evalutationInfos\\Ready\\Boliga_propertySales_evaluationInfo_Ready.csv'\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# select only necessary columns :\n",
    "fields = ['RowID_MAIN_boliga_ROW_ID_unitID', 'RAW_dictionary_evaluationInfos']\n",
    "\n",
    "# Create an iterator for reading chunks\n",
    "chunk_iter = pd.read_csv(file_path, chunksize=chunk_size, usecols=fields)\n",
    "\n",
    "# Initialize a flag to track whether it's the first iteration for the HEADER OF THE RESULTING CSV FILE\n",
    "first_iteration = True\n",
    "\n",
    "# count the chunks \n",
    "chunk_counter = 0\n",
    "\n",
    "# Iterate through each chunk\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    chunk_counter+=1\n",
    "    print(f'----------> chunk nr. {chunk_counter} out of 104 circa')\n",
    "    \n",
    "    # Convert 'RAW_dictionary_evaluationInfos' column to dictionary\n",
    "    # chunk['RAW_dictionary_evaluationInfos'] = chunk['RAW_dictionary_evaluationInfos'].apply(ast.literal_eval)\n",
    "    # chunk['RAW_dictionary_evaluationInfos'] = chunk['RAW_dictionary_evaluationInfos'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.strip() != '' else x)\n",
    "    print('----> Convert to dictionary object')\n",
    "    chunk['RAW_dictionary_evaluationInfos'] = chunk['RAW_dictionary_evaluationInfos'].apply(lambda x: ast.literal_eval(x) if isinstance(x,str) and x != '' and x !='[]' else x)\n",
    "\n",
    "    # Explode the columns so that each item in the list gets its own row\n",
    "    print('----> Explode column')\n",
    "    chunk = chunk.explode('RAW_dictionary_evaluationInfos').reset_index(drop=True)\n",
    "\n",
    "    # Normalize the column into columns and drop then drop the column with pop\n",
    "    print('----> Normalize and join')\n",
    "    chunk = chunk.join(json_normalize(chunk.pop('RAW_dictionary_evaluationInfos')))\n",
    "\n",
    "    # ONLY TAKE VALUATIONS BETWEEN 2006 TO 2024\n",
    "    chunk = chunk[(chunk['evaluationYear'] >= 2006) & (chunk['evaluationYear'] <= 2024)]\n",
    "\n",
    "\n",
    "    # # Expand the dictionary column into rows\n",
    "    # expanded_df = pd.json_normalize(exploded_df['RAW_dictionary_evaluationInfos'])\n",
    "    \n",
    "    # # Concatenate the expanded dataframe with the original dataframe\n",
    "    # result_df = pd.concat([chunk, expanded_df], axis=1)\n",
    "    \n",
    "    # # Drop the original dictionary column\n",
    "    # result_df = result_df.drop(columns=['RAW_dictionary_evaluationInfos'])\n",
    "    \n",
    "    # Write the result to the existing CSV file (mode='a' appends to the existing file)\n",
    "    print('----> Write result to CSV')\n",
    "    chunk.to_csv(result_path, mode='a', index=False, header=first_iteration)\n",
    "\n",
    "    # Update the flag after the first iteration\n",
    "    first_iteration = False\n",
    "\n",
    "    # Release memory by resetting variables and deleting unnecessary objects\n",
    "    # del exploded_df, expanded_df, result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1038900/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent futures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Rename columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_column_names = {\n",
    "    'isSalesValid':'salesInfo_isSalesValid', \n",
    "    'handoverCode':'salesInfo_handoverCode', \n",
    "    'handoverName':'salesInfo_handoverName', \n",
    "    'deedIssueDate':'salesInfo_deedIssueDate', \n",
    "    'price':'salesInfo_price',\n",
    "    'recalculationDate':'salesInfo_recalculationDate', \n",
    "    'rebuildYear':'salesInfo_rebuildYear'    \n",
    "}\n",
    "\n",
    "data_salesInfo = data_salesInfo.rename(columns=change_column_names)\n",
    "data_salesInfo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ Save the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_columns = [\n",
    "    'RowID_MAIN_boliga_ROW_ID_unitID', \n",
    "\n",
    "    'salesInfo_isSalesValid',   \n",
    "    'salesInfo_handoverCode', \n",
    "    'salesInfo_handoverName', \n",
    "    'salesInfo_deedIssueDate', \n",
    "    'salesInfo_price',\n",
    "    'salesInfo_recalculationDate', \n",
    "    'salesInfo_rebuildYear'\n",
    "]\n",
    "\n",
    "path=r'D:\\Thesis\\Properties\\Denmark\\RE_due_scraping_properties\\Boliga_dk\\Creating_main_dataset_for_sales_data\\Data_split\\11_salesInfos\\Ready\\Boliga_propertySales_salesInfos_Ready.csv'\n",
    "\n",
    "data_salesInfo[sub_columns].to_csv(path, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_additionarBuildingInfo.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
